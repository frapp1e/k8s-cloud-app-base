
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMMAND â”‚          ARGS           â”‚ PROFILE  â”‚ USER â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start   â”‚ --driver=docker         â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 23 Feb 26 19:24 CET â”‚                     â”‚
â”‚ start   â”‚ --driver=docker         â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 23 Feb 26 19:28 CET â”‚                     â”‚
â”‚ start   â”‚ --driver=docker         â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 23 Feb 26 19:30 CET â”‚                     â”‚
â”‚ start   â”‚ --driver=docker         â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 23 Feb 26 19:35 CET â”‚ 23 Feb 26 19:36 CET â”‚
â”‚ service â”‚ nginx-test --url        â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 23 Feb 26 23:16 CET â”‚ 23 Feb 26 23:16 CET â”‚
â”‚ service â”‚ cloud-app-service --url â”‚ minikube â”‚ fran â”‚ v1.38.1 â”‚ 24 Feb 26 01:14 CET â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2026/02/23 19:35:45
Running on machine: vbox
Binary: Built with gc go1.25.5 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0223 19:35:45.717645    3955 out.go:360] Setting OutFile to fd 1 ...
I0223 19:35:45.717829    3955 out.go:413] isatty.IsTerminal(1) = true
I0223 19:35:45.717831    3955 out.go:374] Setting ErrFile to fd 2...
I0223 19:35:45.717834    3955 out.go:413] isatty.IsTerminal(2) = true
I0223 19:35:45.717995    3955 root.go:338] Updating PATH: /home/fran/.minikube/bin
W0223 19:35:45.718068    3955 root.go:314] Error reading config file at /home/fran/.minikube/config/config.json: open /home/fran/.minikube/config/config.json: no such file or directory
I0223 19:35:45.718191    3955 out.go:368] Setting JSON to false
I0223 19:35:45.718963    3955 start.go:134] hostinfo: {"hostname":"vbox","uptime":367,"bootTime":1771871379,"procs":263,"os":"linux","platform":"rocky","platformFamily":"rhel","platformVersion":"9.7","kernelVersion":"5.14.0-611.30.1.el9_7.x86_64","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"426b7dc0-cb8c-4e4b-bc3e-08f667801618"}
I0223 19:35:45.719019    3955 start.go:144] virtualization:  
I0223 19:35:45.722187    3955 out.go:179] ðŸ˜„  minikube v1.38.1 en Rocky 9.7
I0223 19:35:45.724729    3955 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.1
I0223 19:35:45.724868    3955 notify.go:220] Checking for updates...
I0223 19:35:45.725111    3955 driver.go:422] Setting default libvirt URI to qemu:///system
I0223 19:35:45.748565    3955 docker.go:125] docker version: linux-29.2.1:Docker Engine - Community
I0223 19:35:45.748684    3955 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0223 19:35:45.807788    3955 info.go:266] docker info: {ID:823fc720-76ee-4409-95fb-cb73bd2abccb Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:42 OomKillDisable:false NGoroutines:65 SystemTime:2026-02-23 19:35:45.800810529 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.14.0-611.30.1.el9_7.x86_64 OperatingSystem:Rocky Linux 9.7 (Blue Onyx) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:5812625408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:vbox Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
I0223 19:35:45.807855    3955 docker.go:320] overlay module found
I0223 19:35:45.810338    3955 out.go:179] âœ¨  Using the docker driver based on existing profile
I0223 19:35:45.811307    3955 start.go:310] selected driver: docker
I0223 19:35:45.811310    3955 start.go:935] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.35.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0223 19:35:45.811349    3955 start.go:946] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0223 19:35:45.811455    3955 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0223 19:35:45.863157    3955 info.go:266] docker info: {ID:823fc720-76ee-4409-95fb-cb73bd2abccb Containers:2 ContainersRunning:2 ContainersPaused:0 ContainersStopped:0 Images:2 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:42 OomKillDisable:false NGoroutines:65 SystemTime:2026-02-23 19:35:45.85619394 +0100 CET LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:5.14.0-611.30.1.el9_7.x86_64 OperatingSystem:Rocky Linux 9.7 (Blue Onyx) OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:6 MemTotal:5812625408 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:vbox Labels:[] ExperimentalBuild:false ServerVersion:29.2.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:dea7da592f5d1d2b7755e3a161be07f43fad8f75 Expected:} RuncCommit:{ID:v1.3.4-0-gd6d73eb8 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.31.1] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v5.0.2]] Warnings:<nil>}}
I0223 19:35:45.863481    3955 cni.go:83] Creating CNI manager for ""
I0223 19:35:45.863512    3955 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0223 19:35:45.863538    3955 start.go:358] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.35.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0223 19:35:45.865411    3955 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0223 19:35:45.866328    3955 cache.go:135] Beginning downloading kic base image for docker with docker
I0223 19:35:45.867168    3955 out.go:179] ðŸšœ  Pulling base image v0.0.50 ...
I0223 19:35:45.868048    3955 preload.go:187] Checking if preload exists for k8s version v1.35.1 and runtime docker
I0223 19:35:45.868067    3955 preload.go:202] Found local preload: /home/fran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.1-docker-overlay2-amd64.tar.lz4
I0223 19:35:45.868070    3955 cache.go:66] Caching tarball of preloaded images
I0223 19:35:45.868111    3955 image.go:82] Checking for gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 in local docker daemon
I0223 19:35:45.868263    3955 preload.go:250] Found /home/fran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0223 19:35:45.868271    3955 cache.go:69] Finished verifying existence of preloaded tar for v1.35.1 on docker
I0223 19:35:45.868317    3955 profile.go:143] Saving config to /home/fran/.minikube/profiles/minikube/config.json ...
I0223 19:35:45.952957    3955 cache.go:164] Downloading gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 to local cache
I0223 19:35:45.953030    3955 image.go:66] Checking for gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 in local cache directory
I0223 19:35:45.953040    3955 image.go:69] Found gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 in local cache directory, skipping pull
I0223 19:35:45.953043    3955 image.go:138] gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 exists in cache, skipping pull
I0223 19:35:45.953049    3955 cache.go:167] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 as a tarball
I0223 19:35:45.953051    3955 cache.go:177] Loading gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 from local cache
I0223 19:36:00.965852    3955 cache.go:179] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 from cached tarball
I0223 19:36:00.965871    3955 cache.go:244] Successfully downloaded all kic artifacts
I0223 19:36:00.965889    3955 start.go:359] acquireMachinesLock for minikube: {Name:mk6c8d01681e10347f1d09802db80e64a14c2dac Timeout:10m0s Delay:500ms}
I0223 19:36:00.965923    3955 start.go:363] duration metric: took 25.215Âµs to acquireMachinesLock for "minikube"
I0223 19:36:00.965932    3955 start.go:95] Skipping create...Using existing machine configuration
I0223 19:36:00.965934    3955 fix.go:53] fixHost starting: 
I0223 19:36:00.966151    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:00.983181    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:00.983202    3955 fix.go:111] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:00.983208    3955 fix.go:116] machineExists: false. err=machine does not exist
I0223 19:36:00.984802    3955 out.go:179] ðŸ¤·  docker "minikube" container is missing, will recreate.
I0223 19:36:00.986772    3955 delete.go:124] DEMOLISHING minikube ...
I0223 19:36:00.986817    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:00.997319    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0223 19:36:00.997337    3955 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:00.997342    3955 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:00.997550    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:01.009355    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:01.009373    3955 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:01.009400    3955 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0223 19:36:01.026025    3955 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0223 19:36:01.026041    3955 kic.go:370] could not find the container minikube to remove it. will try anyways
I0223 19:36:01.026068    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:01.038327    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0223 19:36:01.038348    3955 oci.go:83] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:01.038380    3955 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0223 19:36:01.057473    3955 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0223 19:36:01.057486    3955 oci.go:658] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0223 19:36:02.058665    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:02.088075    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:02.088091    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:02.088093    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:02.088115    3955 retry.go:85] will retry after 600ms: couldn't verify container is exited: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:02.658588    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:02.677574    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:02.677590    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:02.677592    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:03.691095    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:03.741724    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:03.741743    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:03.741745    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:05.239857    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:05.266355    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:05.266386    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:05.266391    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:06.500387    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:06.514147    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:06.514167    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:06.514171    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:09.290691    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:09.307644    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:09.307659    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:09.307662    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:13.443341    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:13.482256    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:13.482276    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:13.482279    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:13.482298    3955 retry.go:85] will retry after 7.8s: couldn't verify container is exited: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:21.322469    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0223 19:36:21.347667    3955 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0223 19:36:21.347682    3955 oci.go:670] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0223 19:36:21.347684    3955 oci.go:672] temporary error: container minikube status is  but expect it to be exited
I0223 19:36:21.347695    3955 oci.go:87] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0223 19:36:21.347722    3955 cli_runner.go:164] Run: docker rm -f -v minikube
I0223 19:36:21.361629    3955 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0223 19:36:21.375341    3955 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0223 19:36:21.375388    3955 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0223 19:36:21.388836    3955 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0223 19:36:21.388896    3955 network_create.go:285] running [docker network inspect minikube] to gather additional debugging logs...
I0223 19:36:21.388904    3955 cli_runner.go:164] Run: docker network inspect minikube
W0223 19:36:21.399257    3955 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0223 19:36:21.399270    3955 network_create.go:288] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0223 19:36:21.399277    3955 network_create.go:290] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0223 19:36:21.399369    3955 fix.go:123] Sleeping 1 second for extra luck!
I0223 19:36:22.429084    3955 start.go:124] createHost starting for "" (driver="docker")
I0223 19:36:22.441336    3955 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=3072MB) ...
I0223 19:36:22.441777    3955 start.go:158] libmachine.API.Create for "minikube" (driver="docker")
I0223 19:36:22.441841    3955 client.go:173] LocalClient.Create starting
I0223 19:36:22.442323    3955 main.go:144] libmachine: Reading certificate data from /home/fran/.minikube/certs/ca.pem
I0223 19:36:22.442488    3955 main.go:144] libmachine: Decoding PEM data...
I0223 19:36:22.442526    3955 main.go:144] libmachine: Parsing certificate...
I0223 19:36:22.442602    3955 main.go:144] libmachine: Reading certificate data from /home/fran/.minikube/certs/cert.pem
I0223 19:36:22.442647    3955 main.go:144] libmachine: Decoding PEM data...
I0223 19:36:22.442672    3955 main.go:144] libmachine: Parsing certificate...
I0223 19:36:22.443455    3955 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0223 19:36:22.479425    3955 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0223 19:36:22.479472    3955 network_create.go:285] running [docker network inspect minikube] to gather additional debugging logs...
I0223 19:36:22.479485    3955 cli_runner.go:164] Run: docker network inspect minikube
W0223 19:36:22.494653    3955 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0223 19:36:22.494670    3955 network_create.go:288] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0223 19:36:22.494677    3955 network_create.go:290] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0223 19:36:22.494724    3955 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0223 19:36:22.506420    3955 network.go:205] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0000b2b80}
I0223 19:36:22.506458    3955 network_create.go:125] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0223 19:36:22.506487    3955 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0223 19:36:22.634668    3955 network_create.go:109] docker network minikube 192.168.49.0/24 created
I0223 19:36:22.634680    3955 kic.go:120] calculated static IP "192.168.49.2" for the "minikube" container
I0223 19:36:22.634780    3955 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0223 19:36:22.669687    3955 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0223 19:36:22.683370    3955 oci.go:102] Successfully created a docker volume minikube
I0223 19:36:22.683456    3955 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 -d /var/lib
I0223 19:36:23.304711    3955 oci.go:106] Successfully prepared a docker volume minikube
I0223 19:36:23.304755    3955 preload.go:187] Checking if preload exists for k8s version v1.35.1 and runtime docker
I0223 19:36:23.304760    3955 kic.go:193] Starting extracting preloaded images to volume ...
I0223 19:36:23.304885    3955 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/fran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 -I lz4 -xf /preloaded.tar -C /extractDir
I0223 19:36:25.548082    3955 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/fran/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.35.1-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 -I lz4 -xf /preloaded.tar -C /extractDir: (2.243155109s)
I0223 19:36:25.548108    3955 kic.go:202] duration metric: took 2.243342807s to extract preloaded images to volume ...
W0223 19:36:25.548351    3955 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W0223 19:36:25.548406    3955 oci.go:251] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I0223 19:36:25.548463    3955 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0223 19:36:25.618098    3955 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3072mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93
I0223 19:36:26.039903    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0223 19:36:26.057065    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:26.071235    3955 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0223 19:36:26.108532    3955 oci.go:143] the created container "minikube" has a running status.
I0223 19:36:26.108549    3955 kic.go:224] Creating ssh key for kic: /home/fran/.minikube/machines/minikube/id_rsa...
I0223 19:36:26.262107    3955 kic_runner.go:190] docker (temp): /home/fran/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0223 19:36:26.288119    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:26.302177    3955 kic_runner.go:92] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0223 19:36:26.302184    3955 kic_runner.go:113] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0223 19:36:26.339312    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:26.351908    3955 machine.go:96] provisionDockerMachine start ...
I0223 19:36:26.351967    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:26.364647    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:26.364859    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:26.364863    3955 main.go:144] libmachine: About to run SSH command:
hostname
I0223 19:36:26.496703    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0223 19:36:26.496741    3955 ubuntu.go:182] provisioning hostname "minikube"
I0223 19:36:26.496799    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:26.511877    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:26.512026    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:26.512031    3955 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0223 19:36:26.642512    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0223 19:36:26.642552    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:26.655550    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:26.655693    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:26.655700    3955 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0223 19:36:26.893902    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0223 19:36:26.893918    3955 ubuntu.go:188] set auth options {CertDir:/home/fran/.minikube CaCertPath:/home/fran/.minikube/certs/ca.pem CaPrivateKeyPath:/home/fran/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/fran/.minikube/machines/server.pem ServerKeyPath:/home/fran/.minikube/machines/server-key.pem ClientKeyPath:/home/fran/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/fran/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/fran/.minikube}
I0223 19:36:26.893927    3955 ubuntu.go:190] setting up certificates
I0223 19:36:26.893935    3955 provision.go:83] configureAuth start
I0223 19:36:26.893969    3955 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 19:36:26.909707    3955 provision.go:142] copyHostCerts
I0223 19:36:26.909769    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/ca.pem --> /home/fran/.minikube/ca.pem (1070 bytes)
I0223 19:36:26.909888    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/cert.pem --> /home/fran/.minikube/cert.pem (1115 bytes)
I0223 19:36:26.910098    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/key.pem --> /home/fran/.minikube/key.pem (1675 bytes)
I0223 19:36:26.910153    3955 provision.go:116] generating server cert: /home/fran/.minikube/machines/server.pem ca-key=/home/fran/.minikube/certs/ca.pem private-key=/home/fran/.minikube/certs/ca-key.pem org=fran.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0223 19:36:26.938577    3955 provision.go:176] copyRemoteCerts
I0223 19:36:26.938715    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0223 19:36:26.938752    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:26.951477    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:27.045888    3955 ssh_runner.go:361] scp /home/fran/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0223 19:36:27.067675    3955 ssh_runner.go:361] scp /home/fran/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0223 19:36:27.089851    3955 ssh_runner.go:361] scp /home/fran/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0223 19:36:27.112227    3955 provision.go:86] duration metric: took 218.282609ms to configureAuth
I0223 19:36:27.112241    3955 ubuntu.go:206] setting minikube options for container-runtime
I0223 19:36:27.112436    3955 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.1
I0223 19:36:27.112484    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:27.135094    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:27.135225    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:27.135229    3955 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0223 19:36:27.261224    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0223 19:36:27.261235    3955 ubuntu.go:71] root file system type: overlay
I0223 19:36:27.261306    3955 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0223 19:36:27.261353    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:27.277048    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:27.277184    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:27.277220    3955 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0223 19:36:27.585676    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0223 19:36:27.585717    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:27.604177    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:27.604310    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:27.604318    3955 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0223 19:36:28.848104    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2026-02-02 17:15:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2026-02-23 18:36:27.583727581 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0223 19:36:28.848122    3955 machine.go:99] duration metric: took 2.496205134s to provisionDockerMachine
I0223 19:36:28.848128    3955 client.go:176] duration metric: took 6.406281459s to LocalClient.Create
I0223 19:36:28.848135    3955 start.go:166] duration metric: took 6.406376435s to libmachine.API.Create "minikube"
I0223 19:36:28.848139    3955 start.go:292] postStartSetup for "minikube" (driver="docker")
I0223 19:36:28.848144    3955 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0223 19:36:28.848236    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0223 19:36:28.848257    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:28.860929    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:28.969706    3955 ssh_runner.go:194] Run: cat /etc/os-release
I0223 19:36:28.972467    3955 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0223 19:36:28.972476    3955 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0223 19:36:28.972481    3955 filesync.go:125] Scanning /home/fran/.minikube/addons for local assets ...
I0223 19:36:28.972515    3955 filesync.go:125] Scanning /home/fran/.minikube/files for local assets ...
I0223 19:36:28.972557    3955 start.go:295] duration metric: took 124.41252ms for postStartSetup
I0223 19:36:28.972710    3955 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 19:36:28.986667    3955 profile.go:143] Saving config to /home/fran/.minikube/profiles/minikube/config.json ...
I0223 19:36:28.986932    3955 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0223 19:36:28.986954    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:28.999696    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:29.092066    3955 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0223 19:36:29.106869    3955 out.go:203] 
W0223 19:36:29.108085    3955 out.go:285] ðŸ§¯  Docker is nearly out of disk space, which may cause deployments to fail! (87% of capacity). You can pass '--force' to skip this check.
W0223 19:36:29.108117    3955 out.go:285] ðŸ’¡  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
W0223 19:36:29.108241    3955 out.go:285] ðŸ¿  Related issue: https://github.com/kubernetes/minikube/issues/9024
I0223 19:36:29.110285    3955 out.go:203] 
I0223 19:36:29.110961    3955 start.go:127] duration metric: took 6.681857686s to createHost
I0223 19:36:29.111008    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:29.123427    3955 machine.go:96] provisionDockerMachine start ...
I0223 19:36:29.123483    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:29.140337    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:29.140517    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:29.140521    3955 main.go:144] libmachine: About to run SSH command:
hostname
I0223 19:36:29.278241    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0223 19:36:29.278264    3955 ubuntu.go:182] provisioning hostname "minikube"
I0223 19:36:29.278392    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:29.304619    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:29.304899    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:29.304905    3955 main.go:144] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0223 19:36:29.442535    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: minikube

I0223 19:36:29.442578    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:29.457211    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:29.457359    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:29.457366    3955 main.go:144] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0223 19:36:29.602032    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0223 19:36:29.602053    3955 ubuntu.go:188] set auth options {CertDir:/home/fran/.minikube CaCertPath:/home/fran/.minikube/certs/ca.pem CaPrivateKeyPath:/home/fran/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/fran/.minikube/machines/server.pem ServerKeyPath:/home/fran/.minikube/machines/server-key.pem ClientKeyPath:/home/fran/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/fran/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/fran/.minikube}
I0223 19:36:29.602072    3955 ubuntu.go:190] setting up certificates
I0223 19:36:29.602083    3955 provision.go:83] configureAuth start
I0223 19:36:29.602158    3955 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 19:36:29.638741    3955 provision.go:142] copyHostCerts
I0223 19:36:29.638788    3955 exec_runner.go:143] found /home/fran/.minikube/ca.pem, removing ...
I0223 19:36:29.638813    3955 exec_runner.go:202] rm: /home/fran/.minikube/ca.pem
I0223 19:36:29.638851    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/ca.pem --> /home/fran/.minikube/ca.pem (1070 bytes)
I0223 19:36:29.638971    3955 exec_runner.go:143] found /home/fran/.minikube/cert.pem, removing ...
I0223 19:36:29.638974    3955 exec_runner.go:202] rm: /home/fran/.minikube/cert.pem
I0223 19:36:29.638988    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/cert.pem --> /home/fran/.minikube/cert.pem (1115 bytes)
I0223 19:36:29.639040    3955 exec_runner.go:143] found /home/fran/.minikube/key.pem, removing ...
I0223 19:36:29.639043    3955 exec_runner.go:202] rm: /home/fran/.minikube/key.pem
I0223 19:36:29.639054    3955 exec_runner.go:150] cp: /home/fran/.minikube/certs/key.pem --> /home/fran/.minikube/key.pem (1675 bytes)
I0223 19:36:29.639100    3955 provision.go:116] generating server cert: /home/fran/.minikube/machines/server.pem ca-key=/home/fran/.minikube/certs/ca.pem private-key=/home/fran/.minikube/certs/ca-key.pem org=fran.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0223 19:36:29.741066    3955 provision.go:176] copyRemoteCerts
I0223 19:36:29.741138    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0223 19:36:29.741160    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:29.757631    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:29.890427    3955 ssh_runner.go:361] scp /home/fran/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0223 19:36:29.907082    3955 ssh_runner.go:361] scp /home/fran/.minikube/machines/server.pem --> /etc/docker/server.pem (1172 bytes)
I0223 19:36:29.920500    3955 ssh_runner.go:361] scp /home/fran/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0223 19:36:29.934189    3955 provision.go:86] duration metric: took 332.098284ms to configureAuth
I0223 19:36:29.934200    3955 ubuntu.go:206] setting minikube options for container-runtime
I0223 19:36:29.934317    3955 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.1
I0223 19:36:29.934347    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:29.946909    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:29.947042    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:29.947046    3955 main.go:144] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0223 19:36:30.085239    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: overlay

I0223 19:36:30.085251    3955 ubuntu.go:71] root file system type: overlay
I0223 19:36:30.085355    3955 provision.go:313] Updating docker unit: /lib/systemd/system/docker.service ...
I0223 19:36:30.085418    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.106932    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:30.107067    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:30.107104    3955 main.go:144] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0223 19:36:30.265320    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I0223 19:36:30.265365    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.279498    3955 main.go:144] libmachine: Using SSH client type: native
I0223 19:36:30.279647    3955 main.go:144] libmachine: &{{{<nil> 0 [] [] []} docker [0x906360] 0x909000 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I0223 19:36:30.279654    3955 main.go:144] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0223 19:36:30.406730    3955 main.go:144] libmachine: SSH cmd err, output: <nil>: 
I0223 19:36:30.406741    3955 machine.go:99] duration metric: took 1.283305442s to provisionDockerMachine
I0223 19:36:30.406748    3955 start.go:292] postStartSetup for "minikube" (driver="docker")
I0223 19:36:30.406755    3955 start.go:321] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0223 19:36:30.406816    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0223 19:36:30.406840    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.418910    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:30.541156    3955 ssh_runner.go:194] Run: cat /etc/os-release
I0223 19:36:30.549045    3955 main.go:144] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0223 19:36:30.549072    3955 info.go:137] Remote host: Debian GNU/Linux 12 (bookworm)
I0223 19:36:30.549086    3955 filesync.go:125] Scanning /home/fran/.minikube/addons for local assets ...
I0223 19:36:30.549150    3955 filesync.go:125] Scanning /home/fran/.minikube/files for local assets ...
I0223 19:36:30.549173    3955 start.go:295] duration metric: took 142.419618ms for postStartSetup
I0223 19:36:30.549313    3955 ssh_runner.go:194] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0223 19:36:30.549353    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.574871    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:30.664829    3955 ssh_runner.go:194] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0223 19:36:30.762268    3955 out.go:203] 
W0223 19:36:30.768437    3955 out.go:285] ðŸ§¯  Docker is nearly out of disk space, which may cause deployments to fail! (87% of capacity). You can pass '--force' to skip this check.
W0223 19:36:30.768651    3955 out.go:285] ðŸ’¡  Suggestion: 

    Try one or more of the following to free up space on the device:
    
    1. Run "docker system prune" to remove unused Docker data (optionally with "-a")
    2. Increase the storage allocated to Docker for Desktop by clicking on:
    Docker icon > Preferences > Resources > Disk Image Size
    3. Run "minikube ssh -- docker system prune" if using the Docker container runtime
W0223 19:36:30.768874    3955 out.go:285] ðŸ¿  Related issue: https://github.com/kubernetes/minikube/issues/9024
I0223 19:36:30.780028    3955 out.go:203] 
I0223 19:36:30.786043    3955 fix.go:55] duration metric: took 29.820082936s for fixHost
I0223 19:36:30.786075    3955 start.go:82] releasing machines lock for "minikube", held for 29.820140761s
I0223 19:36:30.786264    3955 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0223 19:36:30.818558    3955 ssh_runner.go:194] Run: cat /version.json
I0223 19:36:30.818596    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.818867    3955 ssh_runner.go:194] Run: curl -sS -m 2 https://registry.k8s.io/
I0223 19:36:30.819826    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:30.843011    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:30.843632    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:31.052185    3955 ssh_runner.go:194] Run: systemctl --version
I0223 19:36:31.057041    3955 ssh_runner.go:194] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
W0223 19:36:31.060310    3955 cni.go:208] loopback cni configuration skipped: "/etc/cni/net.d/*loopback.conf*" not found
I0223 19:36:31.060338    3955 ssh_runner.go:194] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0223 19:36:31.390452    3955 cni.go:261] disabled [/etc/cni/net.d/10-crio-bridge.conflist.disabled, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0223 19:36:31.390477    3955 start.go:497] detecting cgroup driver to use...
I0223 19:36:31.390630    3955 detect.go:178] detected "systemd" cgroup driver on host os
I0223 19:36:31.390723    3955 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0223 19:36:31.403645    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I0223 19:36:31.410500    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0223 19:36:31.417078    3955 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I0223 19:36:31.417103    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I0223 19:36:31.424076    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0223 19:36:31.431066    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0223 19:36:31.437650    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0223 19:36:31.444276    3955 ssh_runner.go:194] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0223 19:36:31.450243    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0223 19:36:31.456527    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0223 19:36:31.463349    3955 ssh_runner.go:194] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0223 19:36:31.469907    3955 ssh_runner.go:194] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0223 19:36:31.475443    3955 crio.go:165] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 1
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I0223 19:36:31.475463    3955 ssh_runner.go:194] Run: sudo modprobe br_netfilter
I0223 19:36:31.485644    3955 ssh_runner.go:194] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0223 19:36:31.491203    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:31.548287    3955 ssh_runner.go:194] Run: sudo systemctl restart containerd
I0223 19:36:31.619666    3955 start.go:497] detecting cgroup driver to use...
I0223 19:36:31.619690    3955 detect.go:178] detected "systemd" cgroup driver on host os
I0223 19:36:31.619726    3955 ssh_runner.go:194] Run: sudo systemctl cat docker.service
I0223 19:36:31.631133    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0223 19:36:31.644782    3955 ssh_runner.go:194] Run: sudo systemctl stop -f containerd
I0223 19:36:31.667895    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service containerd
I0223 19:36:31.676917    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service crio
I0223 19:36:31.686805    3955 ssh_runner.go:194] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0223 19:36:31.697466    3955 ssh_runner.go:194] Run: which cri-dockerd
I0223 19:36:31.700214    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0223 19:36:31.706646    3955 ssh_runner.go:361] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I0223 19:36:31.716428    3955 ssh_runner.go:194] Run: sudo systemctl unmask docker.service
I0223 19:36:31.776303    3955 ssh_runner.go:194] Run: sudo systemctl enable docker.socket
I0223 19:36:31.836657    3955 docker.go:577] configuring docker to use "systemd" as cgroup driver...
I0223 19:36:31.836725    3955 ssh_runner.go:361] scp memory --> /etc/docker/daemon.json (129 bytes)
I0223 19:36:31.847692    3955 ssh_runner.go:194] Run: sudo systemctl reset-failed docker
I0223 19:36:31.856466    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:31.924995    3955 ssh_runner.go:194] Run: sudo systemctl restart docker
I0223 19:36:32.998286    3955 ssh_runner.go:234] Completed: sudo systemctl restart docker: (1.073271155s)
I0223 19:36:32.998367    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service docker
I0223 19:36:33.012697    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0223 19:36:33.026260    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0223 19:36:33.036490    3955 ssh_runner.go:194] Run: sudo systemctl unmask cri-docker.socket
I0223 19:36:33.127290    3955 ssh_runner.go:194] Run: sudo systemctl enable cri-docker.socket
I0223 19:36:33.220377    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:33.279389    3955 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.socket
I0223 19:36:33.316767    3955 ssh_runner.go:194] Run: sudo systemctl reset-failed cri-docker.service
I0223 19:36:33.349754    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:33.442985    3955 ssh_runner.go:194] Run: sudo systemctl restart cri-docker.service
I0223 19:36:33.499885    3955 ssh_runner.go:194] Run: sudo systemctl is-active --quiet service cri-docker.service
I0223 19:36:33.509560    3955 start.go:554] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0223 19:36:33.509658    3955 ssh_runner.go:194] Run: stat /var/run/cri-dockerd.sock
I0223 19:36:33.512667    3955 start.go:575] Will wait 60s for crictl version
I0223 19:36:33.512708    3955 ssh_runner.go:194] Run: which crictl
I0223 19:36:33.515326    3955 ssh_runner.go:194] Run: sudo /usr/local/bin/crictl version
I0223 19:36:33.532599    3955 start.go:591] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  29.2.1
RuntimeApiVersion:  v1
I0223 19:36:33.532634    3955 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0223 19:36:33.550123    3955 ssh_runner.go:194] Run: docker version --format {{.Server.Version}}
I0223 19:36:33.569006    3955 out.go:252] ðŸ³  Preparando Kubernetes v1.35.1 en Docker 29.2.1...
I0223 19:36:33.569131    3955 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0223 19:36:33.583438    3955 ssh_runner.go:194] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0223 19:36:33.586636    3955 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0223 19:36:33.594392    3955 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false} ...
I0223 19:36:33.594458    3955 preload.go:187] Checking if preload exists for k8s version v1.35.1 and runtime docker
I0223 19:36:33.594484    3955 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0223 19:36:33.608456    3955 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.35.1
registry.k8s.io/kube-scheduler:v1.35.1
registry.k8s.io/kube-proxy:v1.35.1
registry.k8s.io/kube-controller-manager:v1.35.1
registry.k8s.io/etcd:3.6.6-0
registry.k8s.io/coredns/coredns:v1.13.1
registry.k8s.io/pause:3.10.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0223 19:36:33.608463    3955 docker.go:623] Images already preloaded, skipping extraction
I0223 19:36:33.608500    3955 ssh_runner.go:194] Run: docker images --format {{.Repository}}:{{.Tag}}
I0223 19:36:33.625066    3955 docker.go:693] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.35.1
registry.k8s.io/kube-proxy:v1.35.1
registry.k8s.io/kube-controller-manager:v1.35.1
registry.k8s.io/kube-scheduler:v1.35.1
registry.k8s.io/etcd:3.6.6-0
registry.k8s.io/coredns/coredns:v1.13.1
registry.k8s.io/pause:3.10.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0223 19:36:33.625075    3955 cache_images.go:85] Images are preloaded, skipping loading
I0223 19:36:33.625080    3955 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.35.1 docker true true} ...
I0223 19:36:33.625153    3955 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.35.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.35.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0223 19:36:33.625188    3955 ssh_runner.go:194] Run: docker info --format {{.CgroupDriver}}
I0223 19:36:33.672201    3955 cni.go:83] Creating CNI manager for ""
I0223 19:36:33.672210    3955 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0223 19:36:33.684143    3955 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0223 19:36:33.684196    3955 kubeadm.go:196] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.35.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0223 19:36:33.684258    3955 kubeadm.go:202] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.35.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0223 19:36:33.684298    3955 ssh_runner.go:194] Run: sudo ls /var/lib/minikube/binaries/v1.35.1
I0223 19:36:33.690361    3955 binaries.go:50] Found k8s binaries, skipping transfer
I0223 19:36:33.690390    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0223 19:36:33.695981    3955 ssh_runner.go:361] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0223 19:36:33.705949    3955 ssh_runner.go:361] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0223 19:36:33.715862    3955 ssh_runner.go:361] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I0223 19:36:33.725718    3955 ssh_runner.go:194] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0223 19:36:33.728379    3955 ssh_runner.go:194] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0223 19:36:33.735736    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:33.827775    3955 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0223 19:36:33.876874    3955 certs.go:67] Setting up /home/fran/.minikube/profiles/minikube for IP: 192.168.49.2
I0223 19:36:33.876896    3955 certs.go:193] generating shared ca certs ...
I0223 19:36:33.876907    3955 certs.go:225] acquiring lock for ca certs: {Name:mk0bd0addcb2ab4c63bee0b4e222f3072bb50163 Timeout:1m0s Delay:500ms}
I0223 19:36:33.877093    3955 certs.go:239] generating "minikubeCA" ca cert: /home/fran/.minikube/ca.key
I0223 19:36:34.014425    3955 crypto.go:158] Writing cert to /home/fran/.minikube/ca.crt ...
I0223 19:36:34.014435    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/ca.crt: {Name:mkc9f8bf65cc45c2458e45c145d0d94ef387ea6f Timeout:1m0s Delay:500ms}
I0223 19:36:34.014601    3955 crypto.go:166] Writing key to /home/fran/.minikube/ca.key ...
I0223 19:36:34.014605    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/ca.key: {Name:mkf208f244e3ef2619fd81f56d318dce0230d938 Timeout:1m0s Delay:500ms}
I0223 19:36:34.014672    3955 certs.go:239] generating "proxyClientCA" ca cert: /home/fran/.minikube/proxy-client-ca.key
I0223 19:36:34.065247    3955 crypto.go:158] Writing cert to /home/fran/.minikube/proxy-client-ca.crt ...
I0223 19:36:34.065257    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/proxy-client-ca.crt: {Name:mka94ba388534c5374b05e05ff0abf75a8a94ae6 Timeout:1m0s Delay:500ms}
I0223 19:36:34.065400    3955 crypto.go:166] Writing key to /home/fran/.minikube/proxy-client-ca.key ...
I0223 19:36:34.065405    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/proxy-client-ca.key: {Name:mk00dac2ae54dc5af19cee8a0b7533658b453809 Timeout:1m0s Delay:500ms}
I0223 19:36:34.065466    3955 certs.go:255] generating profile certs ...
I0223 19:36:34.065501    3955 certs.go:362] generating signed profile cert for "minikube-user": /home/fran/.minikube/profiles/minikube/client.key
I0223 19:36:34.065509    3955 crypto.go:70] Generating cert /home/fran/.minikube/profiles/minikube/client.crt with IP's: []
I0223 19:36:34.092125    3955 crypto.go:158] Writing cert to /home/fran/.minikube/profiles/minikube/client.crt ...
I0223 19:36:34.092134    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/client.crt: {Name:mk39c4f68e27eec854dad8b453b1df15bf461635 Timeout:1m0s Delay:500ms}
I0223 19:36:34.092267    3955 crypto.go:166] Writing key to /home/fran/.minikube/profiles/minikube/client.key ...
I0223 19:36:34.092272    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/client.key: {Name:mkbce96f638ed83dd4aaa0bda89b829892c1355d Timeout:1m0s Delay:500ms}
I0223 19:36:34.092335    3955 certs.go:362] generating signed profile cert for "minikube": /home/fran/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0223 19:36:34.092346    3955 crypto.go:70] Generating cert /home/fran/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I0223 19:36:34.162160    3955 crypto.go:158] Writing cert to /home/fran/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I0223 19:36:34.162170    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mk4e5e9449e048ebbea19a7ff873b2795a9e6dff Timeout:1m0s Delay:500ms}
I0223 19:36:34.162308    3955 crypto.go:166] Writing key to /home/fran/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I0223 19:36:34.162313    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mk68e56a28e2513165d6c1334f831c7821a7d4cc Timeout:1m0s Delay:500ms}
I0223 19:36:34.162374    3955 certs.go:380] copying /home/fran/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/fran/.minikube/profiles/minikube/apiserver.crt
I0223 19:36:34.162727    3955 certs.go:384] copying /home/fran/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/fran/.minikube/profiles/minikube/apiserver.key
I0223 19:36:34.162952    3955 certs.go:362] generating signed profile cert for "aggregator": /home/fran/.minikube/profiles/minikube/proxy-client.key
I0223 19:36:34.162966    3955 crypto.go:70] Generating cert /home/fran/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I0223 19:36:34.185525    3955 crypto.go:158] Writing cert to /home/fran/.minikube/profiles/minikube/proxy-client.crt ...
I0223 19:36:34.185536    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/proxy-client.crt: {Name:mkf2c56ec17da4db144f21d319aa596ad7bed365 Timeout:1m0s Delay:500ms}
I0223 19:36:34.185715    3955 crypto.go:166] Writing key to /home/fran/.minikube/profiles/minikube/proxy-client.key ...
I0223 19:36:34.185721    3955 lock.go:61] WriteFile acquiring /home/fran/.minikube/profiles/minikube/proxy-client.key: {Name:mkf84f91e828b8ddc3f79bc7b99683fd6731482d Timeout:1m0s Delay:500ms}
I0223 19:36:34.185882    3955 certs.go:482] found cert: /home/fran/.minikube/certs/ca-key.pem (1675 bytes)
I0223 19:36:34.185904    3955 certs.go:482] found cert: /home/fran/.minikube/certs/ca.pem (1070 bytes)
I0223 19:36:34.185918    3955 certs.go:482] found cert: /home/fran/.minikube/certs/cert.pem (1115 bytes)
I0223 19:36:34.185931    3955 certs.go:482] found cert: /home/fran/.minikube/certs/key.pem (1675 bytes)
I0223 19:36:34.186393    3955 ssh_runner.go:361] scp /home/fran/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0223 19:36:34.200727    3955 ssh_runner.go:361] scp /home/fran/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0223 19:36:34.215122    3955 ssh_runner.go:361] scp /home/fran/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0223 19:36:34.229174    3955 ssh_runner.go:361] scp /home/fran/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0223 19:36:34.243271    3955 ssh_runner.go:361] scp /home/fran/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0223 19:36:34.257868    3955 ssh_runner.go:361] scp /home/fran/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0223 19:36:34.272073    3955 ssh_runner.go:361] scp /home/fran/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0223 19:36:34.287436    3955 ssh_runner.go:361] scp /home/fran/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0223 19:36:34.302216    3955 ssh_runner.go:361] scp /home/fran/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0223 19:36:34.316743    3955 ssh_runner.go:361] scp memory --> /var/lib/minikube/kubeconfig (722 bytes)
I0223 19:36:34.327370    3955 ssh_runner.go:194] Run: openssl version
I0223 19:36:34.332057    3955 ssh_runner.go:194] Run: sudo test -s /usr/share/ca-certificates/minikubeCA.pem
I0223 19:36:34.337856    3955 ssh_runner.go:194] Run: sudo ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem
I0223 19:36:34.343941    3955 ssh_runner.go:194] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0223 19:36:34.346863    3955 certs.go:526] hashing: -rw-r--r--. 1 root root 1111 Feb 23 18:36 /usr/share/ca-certificates/minikubeCA.pem
I0223 19:36:34.346914    3955 ssh_runner.go:194] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0223 19:36:34.373314    3955 ssh_runner.go:194] Run: sudo test -L /etc/ssl/certs/b5213941.0
I0223 19:36:34.379183    3955 ssh_runner.go:194] Run: sudo ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0
I0223 19:36:34.385230    3955 ssh_runner.go:194] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0223 19:36:34.389233    3955 certs.go:398] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0223 19:36:34.389297    3955 kubeadm.go:400] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.50@sha256:eb4fec00e8ad70adf8e6436f195cc429825ffb85f95afcdb5d8d9deb576f3e93 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.35.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s Rosetta:false}
I0223 19:36:34.389432    3955 ssh_runner.go:194] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0223 19:36:34.413907    3955 ssh_runner.go:194] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0223 19:36:34.421361    3955 ssh_runner.go:194] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0223 19:36:34.428007    3955 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0223 19:36:34.428065    3955 ssh_runner.go:194] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0223 19:36:34.434157    3955 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0223 19:36:34.434165    3955 kubeadm.go:157] found existing configuration files:

I0223 19:36:34.434194    3955 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0223 19:36:34.440113    3955 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0223 19:36:34.440169    3955 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/admin.conf
I0223 19:36:34.446602    3955 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0223 19:36:34.453037    3955 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0223 19:36:34.453073    3955 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0223 19:36:34.459254    3955 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0223 19:36:34.465626    3955 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0223 19:36:34.465659    3955 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0223 19:36:34.471582    3955 ssh_runner.go:194] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0223 19:36:34.477876    3955 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0223 19:36:34.477906    3955 ssh_runner.go:194] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0223 19:36:34.484178    3955 ssh_runner.go:285] Start: sudo /bin/bash -c "env PATH="/var/lib/minikube/binaries/v1.35.1:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0223 19:36:34.514288    3955 kubeadm.go:318] [init] Using Kubernetes version: v1.35.1
I0223 19:36:34.514324    3955 kubeadm.go:318] [preflight] Running pre-flight checks
I0223 19:36:34.701298    3955 kubeadm.go:318] [preflight] Pulling images required for setting up a Kubernetes cluster
I0223 19:36:34.701356    3955 kubeadm.go:318] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0223 19:36:34.701408    3955 kubeadm.go:318] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0223 19:36:34.716384    3955 kubeadm.go:318] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0223 19:36:34.718813    3955 out.go:252]     â–ª Generando certificados y llaves
I0223 19:36:34.718895    3955 kubeadm.go:318] [certs] Using existing ca certificate authority
I0223 19:36:34.718954    3955 kubeadm.go:318] [certs] Using existing apiserver certificate and key on disk
I0223 19:36:34.765534    3955 kubeadm.go:318] [certs] Generating "apiserver-kubelet-client" certificate and key
I0223 19:36:34.901255    3955 kubeadm.go:318] [certs] Generating "front-proxy-ca" certificate and key
I0223 19:36:35.178124    3955 kubeadm.go:318] [certs] Generating "front-proxy-client" certificate and key
I0223 19:36:35.260066    3955 kubeadm.go:318] [certs] Generating "etcd/ca" certificate and key
I0223 19:36:35.379395    3955 kubeadm.go:318] [certs] Generating "etcd/server" certificate and key
I0223 19:36:35.379633    3955 kubeadm.go:318] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0223 19:36:35.484634    3955 kubeadm.go:318] [certs] Generating "etcd/peer" certificate and key
I0223 19:36:35.484734    3955 kubeadm.go:318] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0223 19:36:35.688088    3955 kubeadm.go:318] [certs] Generating "etcd/healthcheck-client" certificate and key
I0223 19:36:35.864110    3955 kubeadm.go:318] [certs] Generating "apiserver-etcd-client" certificate and key
I0223 19:36:35.925757    3955 kubeadm.go:318] [certs] Generating "sa" key and public key
I0223 19:36:35.926037    3955 kubeadm.go:318] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0223 19:36:36.003626    3955 kubeadm.go:318] [kubeconfig] Writing "admin.conf" kubeconfig file
I0223 19:36:36.055611    3955 kubeadm.go:318] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0223 19:36:36.073594    3955 kubeadm.go:318] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0223 19:36:36.108885    3955 kubeadm.go:318] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0223 19:36:36.150240    3955 kubeadm.go:318] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0223 19:36:36.150565    3955 kubeadm.go:318] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0223 19:36:36.151982    3955 kubeadm.go:318] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0223 19:36:36.154089    3955 out.go:252]     â–ª Iniciando plano de control
I0223 19:36:36.154221    3955 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0223 19:36:36.154328    3955 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0223 19:36:36.154380    3955 kubeadm.go:318] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0223 19:36:36.174944    3955 kubeadm.go:318] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0223 19:36:36.175025    3955 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I0223 19:36:36.180429    3955 kubeadm.go:318] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I0223 19:36:36.180729    3955 kubeadm.go:318] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0223 19:36:36.180756    3955 kubeadm.go:318] [kubelet-start] Starting the kubelet
I0223 19:36:36.266076    3955 kubeadm.go:318] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0223 19:36:36.266147    3955 kubeadm.go:318] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0223 19:36:36.767262    3955 kubeadm.go:318] [kubelet-check] The kubelet is healthy after 501.88913ms
I0223 19:36:36.773479    3955 kubeadm.go:318] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I0223 19:36:36.773714    3955 kubeadm.go:318] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I0223 19:36:36.774010    3955 kubeadm.go:318] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I0223 19:36:36.774158    3955 kubeadm.go:318] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I0223 19:36:38.280062    3955 kubeadm.go:318] [control-plane-check] kube-controller-manager is healthy after 1.506515568s
I0223 19:36:39.476505    3955 kubeadm.go:318] [control-plane-check] kube-scheduler is healthy after 2.703606621s
I0223 19:36:41.276007    3955 kubeadm.go:318] [control-plane-check] kube-apiserver is healthy after 4.502663139s
I0223 19:36:41.291127    3955 kubeadm.go:318] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0223 19:36:41.302652    3955 kubeadm.go:318] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0223 19:36:41.310024    3955 kubeadm.go:318] [upload-certs] Skipping phase. Please see --upload-certs
I0223 19:36:41.310132    3955 kubeadm.go:318] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0223 19:36:41.317977    3955 kubeadm.go:318] [bootstrap-token] Using token: tt4769.wu51iwr4fj98edlo
I0223 19:36:41.319349    3955 out.go:252]     â–ª Configurando reglas RBAC...
I0223 19:36:41.319432    3955 kubeadm.go:318] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0223 19:36:41.324326    3955 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0223 19:36:41.330353    3955 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0223 19:36:41.332407    3955 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0223 19:36:41.334498    3955 kubeadm.go:318] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0223 19:36:41.336338    3955 kubeadm.go:318] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0223 19:36:41.682562    3955 kubeadm.go:318] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0223 19:36:42.100421    3955 kubeadm.go:318] [addons] Applied essential addon: CoreDNS
I0223 19:36:42.684309    3955 kubeadm.go:318] [addons] Applied essential addon: kube-proxy
I0223 19:36:42.685113    3955 kubeadm.go:318] 
I0223 19:36:42.685152    3955 kubeadm.go:318] Your Kubernetes control-plane has initialized successfully!
I0223 19:36:42.685154    3955 kubeadm.go:318] 
I0223 19:36:42.685200    3955 kubeadm.go:318] To start using your cluster, you need to run the following as a regular user:
I0223 19:36:42.685202    3955 kubeadm.go:318] 
I0223 19:36:42.685218    3955 kubeadm.go:318]   mkdir -p $HOME/.kube
I0223 19:36:42.685256    3955 kubeadm.go:318]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0223 19:36:42.685286    3955 kubeadm.go:318]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0223 19:36:42.685288    3955 kubeadm.go:318] 
I0223 19:36:42.685320    3955 kubeadm.go:318] Alternatively, if you are the root user, you can run:
I0223 19:36:42.685321    3955 kubeadm.go:318] 
I0223 19:36:42.685351    3955 kubeadm.go:318]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0223 19:36:42.685352    3955 kubeadm.go:318] 
I0223 19:36:42.685385    3955 kubeadm.go:318] You should now deploy a pod network to the cluster.
I0223 19:36:42.685430    3955 kubeadm.go:318] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0223 19:36:42.685472    3955 kubeadm.go:318]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0223 19:36:42.685474    3955 kubeadm.go:318] 
I0223 19:36:42.685523    3955 kubeadm.go:318] You can now join any number of control-plane nodes by copying certificate authorities
I0223 19:36:42.685583    3955 kubeadm.go:318] and service account keys on each node and then running the following as root:
I0223 19:36:42.685585    3955 kubeadm.go:318] 
I0223 19:36:42.685643    3955 kubeadm.go:318]   kubeadm join control-plane.minikube.internal:8443 --token tt4769.wu51iwr4fj98edlo \
I0223 19:36:42.685705    3955 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:a8b7b91532982024d50bd547b9c9fd1e8200a666232848252c7b11df29907a27 \
I0223 19:36:42.685717    3955 kubeadm.go:318] 	--control-plane 
I0223 19:36:42.685718    3955 kubeadm.go:318] 
I0223 19:36:42.685769    3955 kubeadm.go:318] Then you can join any number of worker nodes by running the following on each as root:
I0223 19:36:42.685770    3955 kubeadm.go:318] 
I0223 19:36:42.685820    3955 kubeadm.go:318] kubeadm join control-plane.minikube.internal:8443 --token tt4769.wu51iwr4fj98edlo \
I0223 19:36:42.685881    3955 kubeadm.go:318] 	--discovery-token-ca-cert-hash sha256:a8b7b91532982024d50bd547b9c9fd1e8200a666232848252c7b11df29907a27 
I0223 19:36:42.686872    3955 kubeadm.go:318] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0223 19:36:42.687037    3955 kubeadm.go:318] 	[WARNING SystemVerification]: kernel release 5.14.0-611.30.1.el9_7.x86_64 is unsupported. Supported LTS versions from the 5.x series are 5.4, 5.10 and 5.15. Any 6.x version is also supported. For cgroups v2 support, the recommended version is 5.10 or newer
I0223 19:36:42.687099    3955 kubeadm.go:318] 	[WARNING Service-kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0223 19:36:42.687109    3955 cni.go:83] Creating CNI manager for ""
I0223 19:36:42.687117    3955 cni.go:157] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0223 19:36:42.688327    3955 out.go:179] ðŸ”—  Configurando CNI bridge CNI ...
I0223 19:36:42.689745    3955 ssh_runner.go:194] Run: sudo mkdir -p /etc/cni/net.d
I0223 19:36:42.696154    3955 ssh_runner.go:361] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0223 19:36:42.706562    3955 ssh_runner.go:194] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0223 19:36:42.706617    3955 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.35.1/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0223 19:36:42.706653    3955 ssh_runner.go:194] Run: sudo /var/lib/minikube/binaries/v1.35.1/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2026_02_23T19_36_42_0700 minikube.k8s.io/version=v1.38.1 minikube.k8s.io/commit=c93a4cb9311efc66b90d33ea03f75f2c4120e9b0 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0223 19:36:42.715532    3955 ops.go:34] apiserver oom_adj: -16
I0223 19:36:42.948487    3955 kubeadm.go:1113] duration metric: took 241.921855ms to wait for elevateKubeSystemPrivileges
I0223 19:36:42.948499    3955 kubeadm.go:402] duration metric: took 8.559209393s to StartCluster
I0223 19:36:42.948510    3955 settings.go:141] acquiring lock: {Name:mk7a76ab7c032302bf1d7103af4c66a7a2b64f42 Timeout:1m0s Delay:500ms}
I0223 19:36:42.948587    3955 settings.go:149] Updating kubeconfig:  /home/fran/.kube/config
I0223 19:36:42.948893    3955 lock.go:61] WriteFile acquiring /home/fran/.kube/config: {Name:mkb709d1857c16adb85b2c3036a70b66738d6d1c Timeout:1m0s Delay:500ms}
I0223 19:36:42.949063    3955 start.go:237] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.35.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0223 19:36:42.949116    3955 addons.go:528] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0223 19:36:42.949157    3955 config.go:183] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.35.1
I0223 19:36:42.949163    3955 addons.go:71] Setting storage-provisioner=true in profile "minikube"
I0223 19:36:42.949174    3955 addons.go:71] Setting default-storageclass=true in profile "minikube"
I0223 19:36:42.949180    3955 addons_storage_classes.go:34] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0223 19:36:42.949184    3955 addons.go:240] Setting addon storage-provisioner=true in "minikube"
I0223 19:36:42.949200    3955 host.go:67] Checking if "minikube" exists ...
I0223 19:36:42.949339    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:42.949420    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:42.950395    3955 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I0223 19:36:42.951649    3955 ssh_runner.go:194] Run: sudo systemctl daemon-reload
I0223 19:36:42.967477    3955 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0223 19:36:42.969326    3955 addons.go:437] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0223 19:36:42.969334    3955 ssh_runner.go:361] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0223 19:36:42.969365    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:42.990331    3955 addons.go:240] Setting addon default-storageclass=true in "minikube"
I0223 19:36:42.990353    3955 host.go:67] Checking if "minikube" exists ...
I0223 19:36:42.990576    3955 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0223 19:36:42.995526    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:43.003577    3955 addons.go:437] installing /etc/kubernetes/addons/storageclass.yaml
I0223 19:36:43.003585    3955 ssh_runner.go:361] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0223 19:36:43.003618    3955 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0223 19:36:43.018085    3955 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/fran/.minikube/machines/minikube/id_rsa Username:docker}
I0223 19:36:43.038723    3955 ssh_runner.go:194] Run: sudo systemctl start kubelet
I0223 19:36:43.049607    3955 api_server.go:51] waiting for apiserver process to appear ...
I0223 19:36:43.049636    3955 ssh_runner.go:194] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0223 19:36:43.059499    3955 api_server.go:71] duration metric: took 110.42236ms to wait for apiserver process to appear ...
I0223 19:36:43.059509    3955 api_server.go:87] waiting for apiserver healthz status ...
I0223 19:36:43.059520    3955 api_server.go:298] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I0223 19:36:43.063494    3955 api_server.go:324] https://192.168.49.2:8443/healthz returned 200:
ok
I0223 19:36:43.068358    3955 api_server.go:140] control plane version: v1.35.1
I0223 19:36:43.068366    3955 api_server.go:130] duration metric: took 8.85434ms to wait for apiserver health ...
I0223 19:36:43.068371    3955 system_pods.go:42] waiting for kube-system pods to appear ...
I0223 19:36:43.073416    3955 system_pods.go:58] 4 kube-system pods found
I0223 19:36:43.073424    3955 system_pods.go:60] "etcd-minikube" [1377e31f-0e33-4246-b8b5-e2c4bb7da444] Running
I0223 19:36:43.073429    3955 system_pods.go:60] "kube-apiserver-minikube" [6ef98c51-2e45-4591-b82a-c876792d83fc] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0223 19:36:43.073437    3955 system_pods.go:60] "kube-controller-manager-minikube" [0e10facc-f3e4-466f-a545-14c2c42dd785] Running
I0223 19:36:43.073441    3955 system_pods.go:60] "kube-scheduler-minikube" [18f32c3d-5f5c-463a-b50d-8831618f915b] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0223 19:36:43.073444    3955 system_pods.go:73] duration metric: took 5.069618ms to wait for pod list to return data ...
I0223 19:36:43.073450    3955 kubeadm.go:586] duration metric: took 124.37558ms to wait for: map[apiserver:true system_pods:true]
I0223 19:36:43.073461    3955 node_conditions.go:101] verifying NodePressure condition ...
I0223 19:36:43.076202    3955 node_conditions.go:121] node storage ephemeral capacity is 17340Mi
I0223 19:36:43.076216    3955 node_conditions.go:122] node cpu capacity is 6
I0223 19:36:43.076233    3955 node_conditions.go:104] duration metric: took 2.769312ms to run NodePressure ...
I0223 19:36:43.076239    3955 start.go:243] waiting for startup goroutines ...
I0223 19:36:43.120313    3955 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0223 19:36:43.125515    3955 ssh_runner.go:194] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.35.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0223 19:36:43.338075    3955 out.go:179] ðŸŒŸ  Complementos habilitados: storage-provisioner, default-storageclass
I0223 19:36:43.339409    3955 addons.go:531] duration metric: took 390.294582ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0223 19:36:43.339428    3955 start.go:248] waiting for cluster config update ...
I0223 19:36:43.339434    3955 start.go:257] writing updated cluster config ...
I0223 19:36:43.339601    3955 ssh_runner.go:194] Run: rm -f paused
I0223 19:36:43.566731    3955 start.go:632] kubectl: 1.35.1, cluster: 1.35.1 (minor skew: 0)
I0223 19:36:43.568165    3955 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Feb 23 18:36:47 minikube cri-dockerd[1488]: time="2026-02-23T18:36:47Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3e212def2973fd978508361321913db9cd8018c36e29857fa50f0549c67bf4fd/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 23 18:36:48 minikube dockerd[1200]: time="2026-02-23T18:36:48.252859095Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=3a2616e0dbf3 ep=k8s_POD_storage-provisioner_kube-system_645c2bbb-6d08-4163-91f4-a1be29028053_0 net=host nid=9d7b3d9e3834
Feb 23 18:36:48 minikube cri-dockerd[1488]: time="2026-02-23T18:36:48Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4bd5736d87d99848bd90d0c90f09c16d74996b6bf78a7539ff95fc579ca49e99/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
Feb 23 18:36:52 minikube cri-dockerd[1488]: time="2026-02-23T18:36:52Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Feb 23 22:11:05 minikube dockerd[1200]: time="2026-02-23T22:11:05.199685502Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=5fdafe39c263 ep=k8s_POD_nginx-test-6ff8854996-97kr2_default_419a03dc-4d12-43dd-b4bb-010b4022d04f_0 net=none nid=ed24d18c0c32
Feb 23 22:11:05 minikube cri-dockerd[1488]: time="2026-02-23T22:11:05Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2dd3ee15efe2a1fd33e821f6dc38ecb9533da0834a7ba0dbfbf8b38d214d7b69/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 22:11:16 minikube cri-dockerd[1488]: time="2026-02-23T22:11:16Z" level=info msg="Pulling image nginx:latest: 47cd406a84ef: Download complete "
Feb 23 22:11:20 minikube cri-dockerd[1488]: time="2026-02-23T22:11:20Z" level=info msg="Stop pulling image nginx:latest: Status: Downloaded newer image for nginx:latest"
Feb 23 22:57:23 minikube dockerd[1200]: time="2026-02-23T22:57:23.546003416Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=8313eeb4a4aa ep=k8s_POD_nginx-test-6ff8854996-5z9v7_default_7b568765-4cb6-42cf-b991-2753beef92b0_0 net=none nid=ed24d18c0c32
Feb 23 22:57:23 minikube dockerd[1200]: time="2026-02-23T22:57:23.548352222Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=35742cd7475a ep=k8s_POD_nginx-test-6ff8854996-lnwkz_default_e5944ac3-0385-4ce8-97ed-1fca762ebe03_0 net=none nid=ed24d18c0c32
Feb 23 22:57:23 minikube cri-dockerd[1488]: time="2026-02-23T22:57:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/670ca905f4524da6a708f59c929fbc0b1de9c4508f9f1f965b5203d51b556ee4/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 22:57:23 minikube cri-dockerd[1488]: time="2026-02-23T22:57:23Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/97c0aa9f6f9444d9bc1b57361637cf108376109c312b93ddf06ccf937811a964/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 22:57:24 minikube cri-dockerd[1488]: time="2026-02-23T22:57:24Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 22:57:25 minikube cri-dockerd[1488]: time="2026-02-23T22:57:25Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 22:58:03 minikube dockerd[1200]: time="2026-02-23T22:58:03.410543561Z" level=info msg="ignoring event" container=741f03bd00103b73f33bcf33769cfc82763eb3dd714fc319564b6c1e6db58abf module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 22:58:03 minikube cri-dockerd[1488]: time="2026-02-23T22:58:03Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"nginx-test-6ff8854996-lnwkz_default\": unexpected command output nsenter: cannot open /proc/58999/ns/net: No such file or directory\n with error: exit status 1"
Feb 23 22:58:03 minikube dockerd[1200]: time="2026-02-23T22:58:03.601853942Z" level=info msg="ignoring event" container=97c0aa9f6f9444d9bc1b57361637cf108376109c312b93ddf06ccf937811a964 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 22:58:04 minikube dockerd[1200]: time="2026-02-23T22:58:04.078538267Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=850234842c71 ep=k8s_POD_nginx-test-6ff8854996-25nks_default_ce8b04a6-fe43-4c8f-85e0-6e9d5e374b9c_0 net=none nid=ed24d18c0c32
Feb 23 22:58:04 minikube cri-dockerd[1488]: time="2026-02-23T22:58:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/515fee33acf053801c3378c969c28ad77a785dcb89f61c75e69b65dfbe4a1f1b/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 22:58:05 minikube cri-dockerd[1488]: time="2026-02-23T22:58:05Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 23:48:00 minikube dockerd[1200]: time="2026-02-23T23:48:00.956889394Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=ffb643f0ec48 ep=k8s_POD_nginx-test-c974946d6-sfq2q_default_98742d5d-453f-482e-bd9a-0a83a858b82a_0 net=none nid=ed24d18c0c32
Feb 23 23:48:00 minikube cri-dockerd[1488]: time="2026-02-23T23:48:00Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/130c22165487125e27fe2f3835d2e19a8a3981c206c8df8ef4bc93b18d2b5860/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:48:07 minikube cri-dockerd[1488]: time="2026-02-23T23:48:07Z" level=info msg="Stop pulling image nginx:1.25: Status: Downloaded newer image for nginx:1.25"
Feb 23 23:48:08 minikube dockerd[1200]: time="2026-02-23T23:48:08.613871064Z" level=info msg="ignoring event" container=5c0189a5014eb24fb59fe0c587433b674418978af8b7e534bc20f23e57804585 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:48:08 minikube dockerd[1200]: time="2026-02-23T23:48:08.825484606Z" level=info msg="ignoring event" container=670ca905f4524da6a708f59c929fbc0b1de9c4508f9f1f965b5203d51b556ee4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:48:08 minikube dockerd[1200]: time="2026-02-23T23:48:08.992222035Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=30aa7b2d45a4 ep=k8s_POD_nginx-test-c974946d6-dh5ck_default_d0aa7671-55e3-4698-b582-5cfeea1d615d_0 net=none nid=ed24d18c0c32
Feb 23 23:48:09 minikube cri-dockerd[1488]: time="2026-02-23T23:48:09Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/7e2be753551ff42d7f57d83ff69ddedcd62e75d2c0ef874e4809d6746520b4c6/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:48:10 minikube cri-dockerd[1488]: time="2026-02-23T23:48:10Z" level=info msg="Stop pulling image nginx:1.25: Status: Image is up to date for nginx:1.25"
Feb 23 23:48:11 minikube dockerd[1200]: time="2026-02-23T23:48:11.724214885Z" level=info msg="ignoring event" container=c243022cdb5d06d72f6e4026e8dfeeae1299e36aa62a736208201ef4cd5c45f6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:48:11 minikube dockerd[1200]: time="2026-02-23T23:48:11.895892326Z" level=info msg="ignoring event" container=515fee33acf053801c3378c969c28ad77a785dcb89f61c75e69b65dfbe4a1f1b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:48:12 minikube dockerd[1200]: time="2026-02-23T23:48:12.108086809Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=c307caf1c176 ep=k8s_POD_nginx-test-c974946d6-vqz7j_default_03029ca5-0374-4811-8965-067e7c5af391_0 net=none nid=ed24d18c0c32
Feb 23 23:48:12 minikube cri-dockerd[1488]: time="2026-02-23T23:48:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/75c5d833d0a531db24d238976f8dac0f2d66473dbd3114c187a6e7ae8b3e83ff/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:48:13 minikube cri-dockerd[1488]: time="2026-02-23T23:48:13Z" level=info msg="Stop pulling image nginx:1.25: Status: Image is up to date for nginx:1.25"
Feb 23 23:48:13 minikube dockerd[1200]: time="2026-02-23T23:48:13.820675983Z" level=info msg="ignoring event" container=203b980816f09df908dded05a0eee66d5b62d68dba43f6aef2ca41358ff8663a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:48:14 minikube dockerd[1200]: time="2026-02-23T23:48:14.039046178Z" level=info msg="ignoring event" container=2dd3ee15efe2a1fd33e821f6dc38ecb9533da0834a7ba0dbfbf8b38d214d7b69 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:38 minikube dockerd[1200]: time="2026-02-23T23:49:38.584546011Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=cbe6538e2977 ep=k8s_POD_nginx-test-6ff8854996-pk24b_default_76039d5d-754d-4ddf-93e6-8f292a624448_0 net=none nid=ed24d18c0c32
Feb 23 23:49:38 minikube cri-dockerd[1488]: time="2026-02-23T23:49:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/297f39481667ee2c1dcf0c6961e4e7762fb9ea086629a798abaaa4d5bb9e1b1f/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:49:39 minikube cri-dockerd[1488]: time="2026-02-23T23:49:39Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 23:49:39 minikube dockerd[1200]: time="2026-02-23T23:49:39.910378705Z" level=info msg="ignoring event" container=5a6d9df7b6fc4407161ff238ff3e522c0135703a8552b30c3c2e7c652610fc8d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:40 minikube dockerd[1200]: time="2026-02-23T23:49:40.144275674Z" level=info msg="ignoring event" container=75c5d833d0a531db24d238976f8dac0f2d66473dbd3114c187a6e7ae8b3e83ff module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:40 minikube dockerd[1200]: time="2026-02-23T23:49:40.284780938Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=e16141ff2513 ep=k8s_POD_nginx-test-6ff8854996-d9jbd_default_0bd52241-7ff3-4f4b-86df-bef08f4f7d9b_0 net=none nid=ed24d18c0c32
Feb 23 23:49:40 minikube cri-dockerd[1488]: time="2026-02-23T23:49:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e603a53c8f642cbe2981bd74995f606095681f76898874956f994c930bfa96ec/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:49:41 minikube cri-dockerd[1488]: time="2026-02-23T23:49:41Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 23:49:41 minikube dockerd[1200]: time="2026-02-23T23:49:41.955724929Z" level=info msg="ignoring event" container=5417a02fd4b1b24af19867e72884b11db25fc0dc5f1cf805e506300a5dfc1593 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:42 minikube dockerd[1200]: time="2026-02-23T23:49:42.152787470Z" level=info msg="ignoring event" container=130c22165487125e27fe2f3835d2e19a8a3981c206c8df8ef4bc93b18d2b5860 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:42 minikube dockerd[1200]: time="2026-02-23T23:49:42.333094051Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=f10fd8ea6313 ep=k8s_POD_nginx-test-6ff8854996-wzkgq_default_4fb5c6c1-7665-4840-8236-c192669909fa_0 net=none nid=ed24d18c0c32
Feb 23 23:49:42 minikube cri-dockerd[1488]: time="2026-02-23T23:49:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ab77b7f0ee12cb8f60deb1e4dfb6a0d71defb61c7c64fc92fa0fbbfca5198394/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 23 23:49:43 minikube cri-dockerd[1488]: time="2026-02-23T23:49:43Z" level=info msg="Stop pulling image nginx:latest: Status: Image is up to date for nginx:latest"
Feb 23 23:49:44 minikube dockerd[1200]: time="2026-02-23T23:49:44.009848978Z" level=info msg="ignoring event" container=47f477fdbc4bd4a9a28ca60bd687f3ed64392bb1a1afa37b2bb6f86573d273de module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 23 23:49:44 minikube dockerd[1200]: time="2026-02-23T23:49:44.171881785Z" level=info msg="ignoring event" container=7e2be753551ff42d7f57d83ff69ddedcd62e75d2c0ef874e4809d6746520b4c6 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.695611428Z" level=info msg="ignoring event" container=3f8bfee399c575b85c4003e24aad604d4a74043861d942eda2182bfcd1f2c855 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.710258073Z" level=info msg="ignoring event" container=bc45967868fd38e22737e33b686a3c7271f93d83ecb474d5fae4fb2983589945 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.719380395Z" level=info msg="ignoring event" container=bfb852d4166a61a8477a4ca154ad55cf7ed4a0d71243da9223b6b8055f09253a module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.905847377Z" level=info msg="ignoring event" container=297f39481667ee2c1dcf0c6961e4e7762fb9ea086629a798abaaa4d5bb9e1b1f module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.942950132Z" level=info msg="ignoring event" container=ab77b7f0ee12cb8f60deb1e4dfb6a0d71defb61c7c64fc92fa0fbbfca5198394 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:03:15 minikube dockerd[1200]: time="2026-02-24T00:03:15.965467834Z" level=info msg="ignoring event" container=e603a53c8f642cbe2981bd74995f606095681f76898874956f994c930bfa96ec module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Feb 24 00:11:53 minikube dockerd[1200]: time="2026-02-24T00:11:53.922171050Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=c520423e15f1 ep=k8s_POD_cloud-app-6c988ccb44-ltzcx_default_7f3b0db6-8d04-4dd2-8eee-04ef97f4c159_0 net=none nid=ed24d18c0c32
Feb 24 00:11:53 minikube dockerd[1200]: time="2026-02-24T00:11:53.924239405Z" level=info msg="sbJoin: gwep4 ''->'', gwep6 ''->''" eid=c8aa9e008480 ep=k8s_POD_cloud-app-6c988ccb44-bxv6g_default_18d5e4cb-d443-42b8-8571-85bbca9df0d2_0 net=none nid=ed24d18c0c32
Feb 24 00:11:53 minikube cri-dockerd[1488]: time="2026-02-24T00:11:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f6b52fcc64ce3b421509a8bf75ce332459221087dfa16e4d2d9073ddde1ef2be/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Feb 24 00:11:53 minikube cri-dockerd[1488]: time="2026-02-24T00:11:53Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0ddbad32ac0b7e1d9b340c2d5e78aae8bc5efbfb565a868c14f1515a4ada3027/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD                                NAMESPACE
865655115ecea       6e38f40d628db       6 hours ago         Running             storage-provisioner       0                   4bd5736d87d99       storage-provisioner                kube-system
3a4a4f53c01c8       aa5e3ebc0dfed       6 hours ago         Running             coredns                   0                   bac7058dd7e63       coredns-7d764666f9-djrkn           kube-system
e5784e90af363       aa5e3ebc0dfed       6 hours ago         Running             coredns                   0                   3e212def2973f       coredns-7d764666f9-q5lkw           kube-system
b06f14cf7885c       6521110cdb017       6 hours ago         Running             kube-proxy                0                   50b6f4ce149ab       kube-proxy-6m7xk                   kube-system
034140eb58750       6f9eeb0cff981       6 hours ago         Running             kube-apiserver            0                   6fef33cab9171       kube-apiserver-minikube            kube-system
70e912b0e5b8b       5f2a969bc7a43       6 hours ago         Running             kube-scheduler            0                   3205536a5e7d1       kube-scheduler-minikube            kube-system
470dffa7a5138       8d7002962c484       6 hours ago         Running             kube-controller-manager   0                   c7abd04aee141       kube-controller-manager-minikube   kube-system
b224b2825fee0       0a108f7189562       6 hours ago         Running             etcd                      0                   af60bce4dc2ad       etcd-minikube                      kube-system


==> coredns [3a4a4f53c01c] <==
maxprocs: Leaving GOMAXPROCS=6: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.13.1
linux/amd64, go1.25.2, 1db4568
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[ERROR] plugin/kubernetes: Failed to watch
[ERROR] plugin/kubernetes: Failed to watch
[ERROR] plugin/kubernetes: Failed to watch


==> coredns [e5784e90af36] <==
maxprocs: Leaving GOMAXPROCS=6: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.13.1
linux/amd64, go1.25.2, 1db4568
[INFO] plugin/ready: Plugins not ready: "kubernetes"
[ERROR] plugin/kubernetes: Failed to watch
[ERROR] plugin/kubernetes: Failed to watch
[ERROR] plugin/kubernetes: Failed to watch


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=c93a4cb9311efc66b90d33ea03f75f2c4120e9b0
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2026_02_23T19_36_42_0700
                    minikube.k8s.io/version=v1.38.1
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Mon, 23 Feb 2026 18:36:39 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 24 Feb 2026 00:14:46 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 24 Feb 2026 00:12:50 +0000   Mon, 23 Feb 2026 18:36:39 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 24 Feb 2026 00:12:50 +0000   Mon, 23 Feb 2026 18:36:39 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 24 Feb 2026 00:12:50 +0000   Mon, 23 Feb 2026 18:36:39 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 24 Feb 2026 00:12:50 +0000   Mon, 23 Feb 2026 18:36:43 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                6
  ephemeral-storage:  17340Mi
  hugepages-2Mi:      0
  memory:             5676392Ki
  pods:               110
Allocatable:
  cpu:                6
  ephemeral-storage:  17340Mi
  hugepages-2Mi:      0
  memory:             5676392Ki
  pods:               110
System Info:
  Machine ID:                 86e0bfeb3f77427722393c2969964edb
  System UUID:                8ff589de-f8ee-4c74-b47f-c8c116a7abe6
  Boot ID:                    187b5e62-8196-4dea-9912-35c963815590
  Kernel Version:             5.14.0-611.30.1.el9_7.x86_64
  OS Image:                   Debian GNU/Linux 12 (bookworm)
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://29.2.1
  Kubelet Version:            v1.35.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (10 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  default                     cloud-app-6c988ccb44-bxv6g          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m2s
  default                     cloud-app-6c988ccb44-ltzcx          0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m2s
  kube-system                 coredns-7d764666f9-djrkn            100m (1%)     0 (0%)      70Mi (1%)        170Mi (3%)     5h38m
  kube-system                 coredns-7d764666f9-q5lkw            100m (1%)     0 (0%)      70Mi (1%)        170Mi (3%)     5h38m
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         5h38m
  kube-system                 kube-apiserver-minikube             250m (4%)     0 (0%)      0 (0%)           0 (0%)         5h38m
  kube-system                 kube-controller-manager-minikube    200m (3%)     0 (0%)      0 (0%)           0 (0%)         5h38m
  kube-system                 kube-proxy-6m7xk                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h38m
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         5h38m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         5h38m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (14%)  0 (0%)
  memory             240Mi (4%)  340Mi (6%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:              <none>


==> dmesg <==
[Feb23 18:29] RETBleed: WARNING: Spectre v2 mitigation leaves CPU vulnerable to RETBleed attacks, data leaks possible!
[  +0.181717] acpi PNP0A03:00: fail to add MMCONFIG information, can't access extended configuration space under this bridge
[  +1.049993] systemd[1]: Invalid DMI field header.
[  +0.324774] device-mapper: core: CONFIG_IMA_DISABLE_HTABLE is disabled. Duplicate IMA measurements will not be recorded in the IMA log.
[  +0.126436] Warning: Unmaintained driver is detected: e1000
[  +0.004608] Warning: Unmaintained driver is detected: e1000_init_module
[  +0.456894] vmwgfx 0000:00:02.0: [drm] *ERROR* vmwgfx seems to be running on an unsupported hypervisor.
[  +0.000002] vmwgfx 0000:00:02.0: [drm] *ERROR* This configuration is likely broken.
[  +0.000002] vmwgfx 0000:00:02.0: [drm] *ERROR* Please switch to a supported graphics device to avoid problems.
[  +2.351192] systemd[1]: Invalid DMI field header.
[  +4.014015] block dm-0: the capability attribute has been deprecated.
[  +1.074596] Warning: Deprecated Driver is detected: nft_compat will not be maintained in a future major release and may be disabled
[  +0.000582] Warning: Deprecated Driver is detected: nft_compat_module_init will not be maintained in a future major release and may be disabled
[  +0.361541] Warning: Deprecated Driver is detected: ip_set will not be maintained in a future major release and may be disabled
[  +0.001251] Warning: Deprecated Driver is detected: ip_set_init will not be maintained in a future major release and may be disabled
[Feb23 18:36] Warning: Deprecated Driver is detected: ip_tables will not be maintained in a future major release and may be disabled
[  +0.001071] Warning: Deprecated Driver is detected: ip_tables_init will not be maintained in a future major release and may be disabled
[Feb23 18:45] hrtimer: interrupt took 2142640 ns
[Feb23 22:15] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 1028824991 wd_nsec: 1028824650
[Feb23 23:01] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 2488406148 wd_nsec: 2488405245


==> etcd [b224b2825fee] <==
{"level":"info","ts":"2026-02-23T23:01:05.846296Z","caller":"traceutil/trace.go:172","msg":"trace[462382381] range","detail":"{range_begin:/registry/persistentvolumeclaims/; range_end:/registry/persistentvolumeclaims0; response_count:0; response_revision:13174; }","duration":"255.005607ms","start":"2026-02-23T23:01:05.591282Z","end":"2026-02-23T23:01:05.846288Z","steps":["trace[462382381] 'agreement among raft nodes before linearized reading'  (duration: 254.931204ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-23T23:01:05.846651Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"179.165061ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/ipaddresses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-23T23:01:05.846722Z","caller":"traceutil/trace.go:172","msg":"trace[389130298] range","detail":"{range_begin:/registry/ipaddresses; range_end:; response_count:0; response_revision:13175; }","duration":"179.222322ms","start":"2026-02-23T23:01:05.667476Z","end":"2026-02-23T23:01:05.846698Z","steps":["trace[389130298] 'agreement among raft nodes before linearized reading'  (duration: 179.152475ms)"],"step_count":1}
{"level":"info","ts":"2026-02-23T23:01:05.846828Z","caller":"traceutil/trace.go:172","msg":"trace[1955132213] transaction","detail":"{read_only:false; response_revision:13175; number_of_response:1; }","duration":"485.697981ms","start":"2026-02-23T23:01:05.361049Z","end":"2026-02-23T23:01:05.846747Z","steps":["trace[1955132213] 'process raft request'  (duration: 485.498734ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-23T23:01:05.847996Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2026-02-23T23:01:05.361035Z","time spent":"485.823723ms","remote":"127.0.0.1:36338","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:13168 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2026-02-23T23:01:39.233157Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":12898}
{"level":"info","ts":"2026-02-23T23:01:39.235356Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":12898,"took":"1.899241ms","hash":2021472382,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1028096,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2026-02-23T23:01:39.235376Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2021472382,"revision":12898,"compact-revision":12658}
{"level":"info","ts":"2026-02-23T23:06:39.248559Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13202}
{"level":"info","ts":"2026-02-23T23:06:39.255916Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13202,"took":"5.824211ms","hash":2689584398,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1073152,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2026-02-23T23:06:39.255938Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2689584398,"revision":13202,"compact-revision":12898}
{"level":"info","ts":"2026-02-23T23:11:39.255607Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13442}
{"level":"info","ts":"2026-02-23T23:11:39.257601Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13442,"took":"1.819544ms","hash":2956367883,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":925696,"current-db-size-in-use":"926 kB"}
{"level":"info","ts":"2026-02-23T23:11:39.257627Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2956367883,"revision":13442,"compact-revision":13202}
{"level":"info","ts":"2026-02-23T23:16:39.270908Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13682}
{"level":"info","ts":"2026-02-23T23:16:39.279050Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13682,"took":"6.753694ms","hash":3969944833,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":925696,"current-db-size-in-use":"926 kB"}
{"level":"info","ts":"2026-02-23T23:16:39.279245Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3969944833,"revision":13682,"compact-revision":13442}
{"level":"info","ts":"2026-02-23T23:21:39.279747Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":13923}
{"level":"info","ts":"2026-02-23T23:21:39.282160Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":13923,"took":"2.174226ms","hash":2035001020,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":942080,"current-db-size-in-use":"942 kB"}
{"level":"info","ts":"2026-02-23T23:21:39.282182Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2035001020,"revision":13923,"compact-revision":13682}
{"level":"info","ts":"2026-02-23T23:26:39.285929Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14161}
{"level":"info","ts":"2026-02-23T23:26:39.288643Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14161,"took":"2.440045ms","hash":897994187,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2026-02-23T23:26:39.288664Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":897994187,"revision":14161,"compact-revision":13923}
{"level":"info","ts":"2026-02-23T23:31:39.293647Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14401}
{"level":"info","ts":"2026-02-23T23:31:39.296246Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14401,"took":"2.307869ms","hash":2889175290,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":909312,"current-db-size-in-use":"909 kB"}
{"level":"info","ts":"2026-02-23T23:31:39.296267Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2889175290,"revision":14401,"compact-revision":14161}
{"level":"warn","ts":"2026-02-23T23:34:56.647329Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"163.126985ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/volumeattachments\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-23T23:34:56.647432Z","caller":"traceutil/trace.go:172","msg":"trace[562461137] range","detail":"{range_begin:/registry/volumeattachments; range_end:; response_count:0; response_revision:14798; }","duration":"163.238275ms","start":"2026-02-23T23:34:56.484169Z","end":"2026-02-23T23:34:56.647407Z","steps":["trace[562461137] 'range keys from in-memory index tree'  (duration: 163.09789ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-23T23:34:56.647308Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"222.049796ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/apiregistration.k8s.io/apiservices\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-23T23:34:56.647482Z","caller":"traceutil/trace.go:172","msg":"trace[513946491] range","detail":"{range_begin:/registry/apiregistration.k8s.io/apiservices; range_end:; response_count:0; response_revision:14798; }","duration":"222.23545ms","start":"2026-02-23T23:34:56.425240Z","end":"2026-02-23T23:34:56.647475Z","steps":["trace[513946491] 'range keys from in-memory index tree'  (duration: 222.010385ms)"],"step_count":1}
{"level":"warn","ts":"2026-02-23T23:34:56.648023Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"176.317228ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2026-02-23T23:34:56.648066Z","caller":"traceutil/trace.go:172","msg":"trace[1178724096] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:14798; }","duration":"176.362242ms","start":"2026-02-23T23:34:56.471699Z","end":"2026-02-23T23:34:56.648061Z","steps":["trace[1178724096] 'range keys from in-memory index tree'  (duration: 176.311386ms)"],"step_count":1}
{"level":"info","ts":"2026-02-23T23:35:00.106524Z","caller":"traceutil/trace.go:172","msg":"trace[29782745] transaction","detail":"{read_only:false; response_revision:14801; number_of_response:1; }","duration":"110.986594ms","start":"2026-02-23T23:34:59.995528Z","end":"2026-02-23T23:35:00.106515Z","steps":["trace[29782745] 'process raft request'  (duration: 110.914311ms)"],"step_count":1}
{"level":"info","ts":"2026-02-23T23:35:14.390224Z","caller":"traceutil/trace.go:172","msg":"trace[356249093] transaction","detail":"{read_only:false; response_revision:14812; number_of_response:1; }","duration":"105.912419ms","start":"2026-02-23T23:35:14.284302Z","end":"2026-02-23T23:35:14.390215Z","steps":["trace[356249093] 'process raft request'  (duration: 105.848459ms)"],"step_count":1}
{"level":"info","ts":"2026-02-23T23:36:39.301651Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14641}
{"level":"info","ts":"2026-02-23T23:36:39.303417Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14641,"took":"1.624103ms","hash":4016076109,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":909312,"current-db-size-in-use":"909 kB"}
{"level":"info","ts":"2026-02-23T23:36:39.303438Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":4016076109,"revision":14641,"compact-revision":14401}
{"level":"info","ts":"2026-02-23T23:41:39.305977Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":14881}
{"level":"info","ts":"2026-02-23T23:41:39.308162Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":14881,"took":"1.931313ms","hash":2996787509,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2026-02-23T23:41:39.308184Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2996787509,"revision":14881,"compact-revision":14641}
{"level":"info","ts":"2026-02-23T23:46:39.334785Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15121}
{"level":"info","ts":"2026-02-23T23:46:39.337144Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15121,"took":"1.940096ms","hash":3202832444,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":905216,"current-db-size-in-use":"905 kB"}
{"level":"info","ts":"2026-02-23T23:46:39.337165Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3202832444,"revision":15121,"compact-revision":14881}
{"level":"info","ts":"2026-02-23T23:51:39.357895Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15359}
{"level":"info","ts":"2026-02-23T23:51:39.362115Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15359,"took":"3.805084ms","hash":2774961919,"current-db-size-bytes":1458176,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1458176,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2026-02-23T23:51:39.362179Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2774961919,"revision":15359,"compact-revision":15121}
{"level":"info","ts":"2026-02-23T23:56:39.375176Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":15822}
{"level":"info","ts":"2026-02-23T23:56:39.387837Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":15822,"took":"10.821696ms","hash":2192449727,"current-db-size-bytes":1589248,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1589248,"current-db-size-in-use":"1.6 MB"}
{"level":"info","ts":"2026-02-23T23:56:39.388346Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2192449727,"revision":15822,"compact-revision":15359}
{"level":"info","ts":"2026-02-23T23:56:56.055363Z","caller":"etcdserver/server.go:2201","msg":"triggering snapshot","local-member-id":"aec36adc501070cc","local-member-applied-index":20002,"local-member-snapshot-index":10001,"local-member-snapshot-count":10000,"snapshot-forced":false}
{"level":"info","ts":"2026-02-23T23:56:56.061144Z","caller":"etcdserver/server.go:2246","msg":"saved snapshot to disk","snapshot-index":20002}
{"level":"info","ts":"2026-02-24T00:01:39.386541Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16062}
{"level":"info","ts":"2026-02-24T00:01:39.388823Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16062,"took":"1.883295ms","hash":3325532237,"current-db-size-bytes":1589248,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1011712,"current-db-size-in-use":"1.0 MB"}
{"level":"info","ts":"2026-02-24T00:01:39.388853Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":3325532237,"revision":16062,"compact-revision":15822}
{"level":"info","ts":"2026-02-24T00:06:39.396131Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16302}
{"level":"info","ts":"2026-02-24T00:06:39.400919Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16302,"took":"4.355452ms","hash":2713263576,"current-db-size-bytes":1589248,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1097728,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2026-02-24T00:06:39.400991Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":2713263576,"revision":16302,"compact-revision":16062}
{"level":"info","ts":"2026-02-24T00:11:39.409613Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":16581}
{"level":"info","ts":"2026-02-24T00:11:39.411828Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":16581,"took":"1.862506ms","hash":160760265,"current-db-size-bytes":1589248,"current-db-size":"1.6 MB","current-db-size-in-use-bytes":1085440,"current-db-size-in-use":"1.1 MB"}
{"level":"info","ts":"2026-02-24T00:11:39.411850Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":160760265,"revision":16581,"compact-revision":16302}


==> kernel <==
 00:14:55 up  5:45,  0 user,  load average: 0.50, 0.37, 0.38
Linux minikube 5.14.0-611.30.1.el9_7.x86_64 #1 SMP PREEMPT_DYNAMIC Fri Feb 13 17:04:55 UTC 2026 x86_64 GNU/Linux
PRETTY_NAME="Debian GNU/Linux 12 (bookworm)"


==> kube-apiserver [034140eb5875] <==
I0223 18:36:39.525816       1 controller.go:667] quota admission added evaluator for: namespaces
E0223 18:36:39.528192       1 controller.go:201] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0223 18:36:39.534220       1 default_servicecidr_controller.go:231] Setting default ServiceCIDR condition Ready to True
I0223 18:36:39.539655       1 cidrallocator.go:302] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 18:36:39.541688       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 18:36:39.746099       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0223 18:36:40.445211       1 storage_scheduling.go:123] created PriorityClass system-node-critical with value 2000001000
I0223 18:36:40.455215       1 storage_scheduling.go:123] created PriorityClass system-cluster-critical with value 2000000000
I0223 18:36:40.455242       1 storage_scheduling.go:139] all system priority classes are created successfully or already exist.
I0223 18:36:40.777295       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0223 18:36:40.801684       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0223 18:36:40.931717       1 alloc.go:329] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0223 18:36:40.936216       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0223 18:36:40.936925       1 controller.go:667] quota admission added evaluator for: endpoints
I0223 18:36:40.941249       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0223 18:36:41.271483       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0223 18:36:42.089989       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0223 18:36:42.099640       1 alloc.go:329] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0223 18:36:42.105761       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0223 18:36:47.064540       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 18:36:47.069409       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 18:36:47.210924       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I0223 18:36:47.260524       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0223 18:46:39.432683       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 18:56:39.434981       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:06:39.435208       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:16:39.436254       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:26:39.436991       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:36:39.439122       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:46:39.441147       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 19:56:39.441740       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:06:39.442432       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:16:39.444993       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:26:39.445041       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:36:39.446047       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:46:39.447359       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 20:56:39.458124       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:06:39.462052       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:16:39.465192       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:26:39.464313       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:36:39.465015       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:46:39.466790       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 21:56:39.471593       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:06:39.474350       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:11:21.140028       1 alloc.go:329] "allocated clusterIPs" service="default/nginx-test" clusterIPs={"IPv4":"10.108.126.73"}
I0223 22:16:39.474781       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:26:39.476259       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:36:39.476447       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:46:39.477350       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 22:56:39.478818       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:06:39.480531       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:16:39.480533       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:26:39.481650       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:36:39.484264       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:46:39.484952       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0223 23:56:39.485530       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0224 00:03:26.212263       1 alloc.go:329] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0224 00:03:26.217999       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0224 00:06:39.486884       1 cidrallocator.go:278] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0224 00:14:18.890382       1 alloc.go:329] "allocated clusterIPs" service="default/cloud-app-service" clusterIPs={"IPv4":"10.111.232.142"}


==> kube-controller-manager [470dffa7a513] <==
I0223 18:36:45.980479       1 graph_builder.go:386] "Running" component="GraphBuilder"
I0223 18:36:45.983349       1 shared_informer.go:370] "Waiting for caches to sync"
I0223 18:36:45.983469       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0223 18:36:46.077447       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.077550       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.077625       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.077767       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.078109       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.078237       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.078537       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.078917       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.078945       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.079057       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.079381       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.079776       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.079961       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080022       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080502       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080679       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080708       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080730       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080740       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080760       1 range_allocator.go:177] "Sending events to api server"
I0223 18:36:46.080788       1 range_allocator.go:181] "Starting range CIDR allocator"
I0223 18:36:46.080768       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080830       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080863       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" zone=""
I0223 18:36:46.080905       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I0223 18:36:46.080923       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" zone="" newState="Normal"
I0223 18:36:46.080955       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.080962       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081008       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081021       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081027       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081037       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081044       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081061       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081169       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081038       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081211       1 shared_informer.go:370] "Waiting for caches to sync"
I0223 18:36:46.081954       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081400       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081407       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081412       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081417       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081423       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081427       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081432       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081437       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081463       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.081467       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.086051       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.086325       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.087602       1 range_allocator.go:433] "Set node PodCIDR" node="minikube" podCIDRs=["10.244.0.0/24"]
I0223 18:36:46.088913       1 shared_informer.go:370] "Waiting for caches to sync"
I0223 18:36:46.181150       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:46.181292       1 garbagecollector.go:166] "Garbage collector: all resource monitors have synced"
I0223 18:36:46.181327       1 garbagecollector.go:169] "Proceeding to collect garbage"
I0223 18:36:46.190890       1 shared_informer.go:377] "Caches are synced"
I0223 20:36:45.985726       1 cleaner.go:189] "Cleaning CSR as it is more than approvedExpiration duration old and approved." csr="csr-dvdbw" approvedExpiration="1h0m0s"


==> kube-proxy [b06f14cf7885] <==
I0223 18:36:48.032304       1 server_linux.go:53] "Using iptables proxy"
I0223 18:36:48.129420       1 shared_informer.go:370] "Waiting for caches to sync"
I0223 18:36:48.230361       1 shared_informer.go:377] "Caches are synced"
I0223 18:36:48.230383       1 server.go:218] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E0223 18:36:48.230436       1 server.go:255] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0223 18:36:48.257911       1 server.go:264] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0223 18:36:48.257959       1 server_linux.go:136] "Using iptables Proxier"
I0223 18:36:48.262695       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0223 18:36:48.264340       1 proxier.go:270] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0223 18:36:48.266465       1 proxier.go:270] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0223 18:36:48.267407       1 server.go:529] "Version info" version="v1.35.1"
I0223 18:36:48.267419       1 server.go:531] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0223 18:36:48.270435       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0223 18:36:48.272167       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0223 18:36:48.274448       1 config.go:403] "Starting serviceCIDR config controller"
I0223 18:36:48.274461       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I0223 18:36:48.274495       1 config.go:200] "Starting service config controller"
I0223 18:36:48.274503       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I0223 18:36:48.274522       1 config.go:309] "Starting node config controller"
I0223 18:36:48.274529       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I0223 18:36:48.274531       1 shared_informer.go:356] "Caches are synced" controller="node config"
I0223 18:36:48.274932       1 config.go:106] "Starting endpoint slice config controller"
I0223 18:36:48.274953       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I0223 18:36:48.375239       1 shared_informer.go:356] "Caches are synced" controller="service config"
I0223 18:36:48.375305       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"
I0223 18:36:48.375387       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"


==> kube-scheduler [70e912b0e5b8] <==
I0223 18:36:38.331850       1 serving.go:386] Generated self-signed cert in-memory
W0223 18:36:39.432832       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0223 18:36:39.432858       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0223 18:36:39.432876       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0223 18:36:39.432880       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0223 18:36:39.471058       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.35.1"
I0223 18:36:39.471078       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0223 18:36:39.472467       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0223 18:36:39.472530       1 shared_informer.go:370] "Waiting for caches to sync"
I0223 18:36:39.472545       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0223 18:36:39.472535       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
E0223 18:36:39.475576       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1693" type="*v1.ConfigMap"
E0223 18:36:39.477693       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PodDisruptionBudget"
E0223 18:36:39.478713       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSINode"
E0223 18:36:39.478732       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Namespace"
E0223 18:36:39.479037       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ReplicaSet"
E0223 18:36:39.479062       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIStorageCapacity"
E0223 18:36:39.479690       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSIDriver"
E0223 18:36:39.479706       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Service"
E0223 18:36:39.479889       1 reflector.go:204] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.VolumeAttachment"
E0223 18:36:39.479980       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Pod"
E0223 18:36:39.479989       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ReplicationController"
E0223 18:36:39.480053       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StatefulSet"
E0223 18:36:39.480080       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolumeClaim"
E0223 18:36:39.480090       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StorageClass"
E0223 18:36:39.480057       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Node"
E0223 18:36:39.480216       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceClaim"
E0223 18:36:39.480402       1 reflector.go:204] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.PersistentVolume"
E0223 18:36:39.480592       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceSlice"
E0223 18:36:39.480623       1 reflector.go:204] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.DeviceClass"
E0223 18:36:40.318266       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StatefulSet"
E0223 18:36:40.358915       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Node"
E0223 18:36:40.431811       1 reflector.go:204] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.VolumeAttachment"
E0223 18:36:40.476039       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceClaim"
E0223 18:36:40.487304       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.ResourceSlice"
E0223 18:36:40.519294       1 reflector.go:204] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.CSINode"
E0223 18:36:40.546602       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Pod"
E0223 18:36:40.562063       1 reflector.go:204] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.DeviceClass"
E0223 18:36:40.619091       1 reflector.go:204] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.StorageClass"
E0223 18:36:40.656955       1 reflector.go:204] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:161" type="*v1.Service"
E0223 18:36:40.803190       1 reflector.go:204] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1693" type="*v1.ConfigMap"
I0223 18:36:43.773305       1 shared_informer.go:377] "Caches are synced"


==> kubelet <==
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.007217    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.007248    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.040151    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.040187    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.050054    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:11:54 minikube kubelet[2316]: E0224 00:11:54.050144    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:05 minikube kubelet[2316]: E0224 00:12:05.884673    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:05 minikube kubelet[2316]: E0224 00:12:05.884722    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:05 minikube kubelet[2316]: E0224 00:12:05.884721    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:12:05 minikube kubelet[2316]: E0224 00:12:05.886414    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:16 minikube kubelet[2316]: E0224 00:12:16.886395    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:16 minikube kubelet[2316]: E0224 00:12:16.886429    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:20 minikube kubelet[2316]: E0224 00:12:20.886654    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:20 minikube kubelet[2316]: E0224 00:12:20.887158    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:12:28 minikube kubelet[2316]: E0224 00:12:28.885233    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:28 minikube kubelet[2316]: E0224 00:12:28.885266    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:34 minikube kubelet[2316]: E0224 00:12:34.886432    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:34 minikube kubelet[2316]: E0224 00:12:34.886498    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:12:42 minikube kubelet[2316]: E0224 00:12:42.888578    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:42 minikube kubelet[2316]: E0224 00:12:42.888679    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:46 minikube kubelet[2316]: E0224 00:12:46.884696    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:46 minikube kubelet[2316]: E0224 00:12:46.884744    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:12:56 minikube kubelet[2316]: E0224 00:12:56.886461    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:56 minikube kubelet[2316]: E0224 00:12:56.886495    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:12:57 minikube kubelet[2316]: E0224 00:12:57.885328    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:12:57 minikube kubelet[2316]: E0224 00:12:57.885705    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:13:10 minikube kubelet[2316]: E0224 00:13:10.884395    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:10 minikube kubelet[2316]: E0224 00:13:10.884769    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:13:10 minikube kubelet[2316]: E0224 00:13:10.884448    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:10 minikube kubelet[2316]: E0224 00:13:10.886005    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:13:24 minikube kubelet[2316]: E0224 00:13:24.883413    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:24 minikube kubelet[2316]: E0224 00:13:24.883448    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:13:25 minikube kubelet[2316]: E0224 00:13:25.892357    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:25 minikube kubelet[2316]: E0224 00:13:25.892400    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:13:36 minikube kubelet[2316]: E0224 00:13:36.884550    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:36 minikube kubelet[2316]: E0224 00:13:36.884598    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:13:40 minikube kubelet[2316]: E0224 00:13:40.886123    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:40 minikube kubelet[2316]: E0224 00:13:40.886198    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:13:50 minikube kubelet[2316]: E0224 00:13:50.888243    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:50 minikube kubelet[2316]: E0224 00:13:50.888334    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:13:52 minikube kubelet[2316]: E0224 00:13:52.891098    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:13:52 minikube kubelet[2316]: E0224 00:13:52.891145    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:14:03 minikube kubelet[2316]: E0224 00:14:03.885546    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:03 minikube kubelet[2316]: E0224 00:14:03.885597    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:14:05 minikube kubelet[2316]: E0224 00:14:05.889127    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:05 minikube kubelet[2316]: E0224 00:14:05.889169    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:14:17 minikube kubelet[2316]: E0224 00:14:17.897512    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:17 minikube kubelet[2316]: E0224 00:14:17.897564    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:14:17 minikube kubelet[2316]: E0224 00:14:17.897696    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:17 minikube kubelet[2316]: E0224 00:14:17.899167    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:14:30 minikube kubelet[2316]: E0224 00:14:30.906839    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:30 minikube kubelet[2316]: E0224 00:14:30.908222    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:30 minikube kubelet[2316]: E0224 00:14:30.908120    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:14:30 minikube kubelet[2316]: E0224 00:14:30.909427    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:14:41 minikube kubelet[2316]: E0224 00:14:41.886044    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:41 minikube kubelet[2316]: E0224 00:14:41.886192    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"
Feb 24 00:14:45 minikube kubelet[2316]: E0224 00:14:45.884417    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-bxv6g_default(18d5e4cb-d443-42b8-8571-85bbca9df0d2): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:45 minikube kubelet[2316]: E0224 00:14:45.884461    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-bxv6g" podUID="18d5e4cb-d443-42b8-8571-85bbca9df0d2"
Feb 24 00:14:54 minikube kubelet[2316]: E0224 00:14:54.883592    2316 kuberuntime_manager.go:1664] "Unhandled Error" err="container cloud-app start failed in pod cloud-app-6c988ccb44-ltzcx_default(7f3b0db6-8d04-4dd2-8eee-04ef97f4c159): ErrImageNeverPull: Container image \"cloud-app:v1\" is not present with pull policy of Never" logger="UnhandledError"
Feb 24 00:14:54 minikube kubelet[2316]: E0224 00:14:54.883623    2316 pod_workers.go:1324] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"cloud-app\" with ErrImageNeverPull: \"Container image \\\"cloud-app:v1\\\" is not present with pull policy of Never\"" pod="default/cloud-app-6c988ccb44-ltzcx" podUID="7f3b0db6-8d04-4dd2-8eee-04ef97f4c159"


==> storage-provisioner [865655115ece] <==
W0224 00:13:56.236004       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:13:56.243957       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:13:58.246634       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:13:58.250420       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:00.255614       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:00.258446       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:02.270342       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:02.284567       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:04.292144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:04.295144       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:06.298236       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:06.301455       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:08.308165       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:08.314536       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:10.321101       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:10.330910       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:12.334301       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:12.338376       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:14.340259       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:14.342986       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:16.345834       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:16.348436       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:18.352736       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:18.358964       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:20.365440       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:20.369376       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:22.372323       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:22.375126       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:24.378286       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:24.382968       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:26.386988       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:26.392552       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:28.402820       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:28.412879       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:30.416718       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:30.419686       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:32.423403       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:32.427042       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:34.439670       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:34.459183       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:36.472363       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:36.485598       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:38.497167       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:38.509731       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:40.522559       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:40.539617       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:42.543216       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:42.546616       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:44.560573       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:44.563436       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:46.568746       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:46.578300       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:48.585226       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:48.588617       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:50.590538       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:50.593651       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:52.604003       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:52.608054       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:54.616712       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0224 00:14:54.627217       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

